# Ensemble(앙상블)
- 보팅(Voting)
- 배깅(Bagging)
- 부스팅(Boosting)
- 스태킹(Stacking)


## 보팅(Voting)
- 여러 개의 분류기가 투표를 통해 최종 예측 결과를 결정하는 방식
- 다른 유형의 알고리즘 기반 분류기 사용
- 하드 보팅(Hard Voting)
    - 다수 분류기가 예측한 값이 최종 결괏값
    - 분류
- 소프트 보팅(Soft Voting)
    - 모든 분류기가 예측한 레이블 값의 결정 확률 평균을 구한 뒤 가장 확률이 높은 레이블 값을 최종 결과로 선정
    - 평균

<img width="366" alt="image" src="https://user-images.githubusercontent.com/63540952/158159836-71bc1430-6e13-4efb-abad-73d855a87b8f.png">


<img width="785" alt="image" src="https://user-images.githubusercontent.com/63540952/158159883-cc0ab7be-444e-4adc-a05a-f3578d57b8c4.png">

## 배깅(bagging)
- Bootstrap Aggregation의 약자
- 데이터로부터 부트스트랩 한 데이터로 모델을 학습시킨 후, 학습된 모델의 결과를 집계하여 최종 결과를 얻는 방법
- 같은 유ㅠ형의 알고리즘 기반 분류기 사용
- 데이터 분할 시 중복을 허용(복원 랜덤 샘플링 방식)
- 범주형 데이터는 투표방식으로 결과를 집계
- 연속형 데이터는 평균으로 결과를 집계

<img width="512" alt="image" src="https://user-images.githubusercontent.com/63540952/158160329-f2efc62e-e8bf-405d-8b14-1d3f09f432c7.png">

## 부스팅(Boosting)
- 같은 유형의 알고리즘 기반 분류기 여러 개에 대해 순차적으로 학습 수행
- 이전 분류기 예측이 틀린 데이터에 대해 올바르게 예측할 수 있도록 다음 분류기에게 가중치를 부여하면서 학습과 예측을 진행
- 계속하여 분류기에게 가중치를 부스팅하며 학습을 진행해 부스팅 방식이라 함
- 예측 성능이 뛰어나 앙상블 학습을 주도함
- 배깅에 비해 성능이 좋지만, 속도가 느리고 과적합 발생 가능성이 있음 -> 상황에 맞게 적절히 사용
- 배깅은 분류기들이 학습 시 상호 영향을 주지않고 학습이 끝난 후 그 결과를 종합하는 방법이며, 수브팅은 이전 분류기 학습 결과를 토대로 다음 분류기 학습 데이터 샘플 가중치를 조정해 학습을 진행하는 방법
- 대표적인 부스팅방법 : XGBoost, LightGBM
<img width="687" alt="image" src="https://user-images.githubusercontent.com/63540952/158165473-dff45fe6-2647-4031-8f38-523ea5ab16db.png">

## 스태킹(Stacking)
- 여러 모델의 예측 값을 최종 모델의 학습 데이터로 사용하여 예측하는 방법
- 현실 모델에서 많이 사용되지 않음
- 캐글 같은 미세한 성능차이로 승부를 결정하는 대회에서 사용됨
- 기본 모델로 4개 이상 선택해야 좋은 결과를 기대할 수 있음

<img width="803" alt="image" src="https://user-images.githubusercontent.com/63540952/158165256-eec10fe9-7e64-490d-b221-66954734945e.png">

# Random Forest
- 여러 의사결정 트리(Decision Tree)를 배깅해서 예측을 실행하는 모델
- 여러 나무들이 모여있다는 의미로 랜덤 포레스트라 붙여짐
- 기본적으로 여러 의사결정 트리의 묶음이라고 보면됨
- 의사결정 트리 주요 단점 : 학습 데이털에 과대적합되는 경향이 있음 -> 랜덤포레스트가 이 문제를 회피할 수 있는 방법이됨
- 서로 다른 방향으로 과대적합된 트리를 많이 만들고, 그결과를 평균 내어 과대적합된 양을 줄임

### 장점
- 의사결정 트리의 쉽고 직관적인 장점을 그대로 가지고 있음
- 앙상블 알고리즘 중 수행속도가 비교적 빠름
- 다양한 분야에서 좋은 성능을 나타냄

### 단점
- 하이퍼파라미터가 많아 튜닝을 위해 많은 시간이 소요됨

#### Hyperparameter
- m_estimators : 트리의 개수
- max_features : Split대상 Feature 개수

# XGBoost
- Extreme Gradient Boosting
- Boosting 기법을 이용하여 구현한 알고리즘은 Gradient Boost가 대표적
- Gradient Boost 알고리즘을 병렬학습이 가능하도록 구현한 것이 XGBoost
- 회귀, 분류 문제를 모두 지원하며, 성능과 자원 효율이 좋아 많이 사용됨

### 장점
- 병렬 처리로 학슶과 예측 속도가 GBM 보다 빠름
- 과적합 규제가 가능하여 강한 내구성을 지님
- 분류와 회귀영역에서 뛰어난 예측 성능 발휘
- 조기 종료 기능을 가짐
- 다양한 옵션을 제공하여 커스터마이징 용이
