# Decision Tree

- 결정트리, 의사 결정 트리, 의사 결정 나무 등으로 번역
- 특정 변수에 대한 의사 결정 규칙을 나무 가지가 뻗는 형태로 분류
- 분류와 회귀 모두에 사용되는 정확도가 높은 지도학습 알고리즘
- 분석 과정이 직관적이며, 이해하기 쉽고 설명하기도 쉬움
- 분석과정을 실제로 눈으로 확인할 수 있어서 대표적인 화이트박스 모델
- 스무고개 알고리즘과 매우 비슷 -> 데이터를 연속적으로 분리하다 보면 결국 하나의 정답으로 분류할 수 있게 됨
- 의미있는 질문을 먼저 하는 것이 중요
- 계산 비용이 낮아 대규모의 데이터 셋에서도 비교적 빠르게 연산 가능


## 불순도(Impurity)
- 불순도를 수치화 할 수 있는지표
    - 지니 불순도(Gini Impourity)
    - 엔트로피(Entropy)


### 지니 불순도(Gini Impurity)
<img width="551" alt="스크린샷 2022-03-10 오전 1 19 20" src="https://user-images.githubusercontent.com/63540952/157484135-5af9971e-5f5e-4e28-880a-103aadee1fca.png">

- 분류 후에 얼마나 잘 분류했는지 평가하는 지표
- 특징이 이진 분류로 나뉠 때 사용됨
- 지니 불순도가 낮을수록 순도가 높음
- 지니 불순도는 0 ~ 0.5 사이의 값
    - 순수하게(완벽) 분류되며 -> 0
    - 완벽하게 섞이면(50:50) -> 0.5
- 지니 불순도가 낮은 속성으로 의사 결정 트리 노드 결정



### 엔트로피(Entropy)
<img width="511" alt="스크린샷 2022-03-10 오전 1 20 18" src="https://user-images.githubusercontent.com/63540952/157484071-f1603489-c48e-4967-b331-888cf5fdbf9c.png">

- p_i : 집합 안에서 속성 i의 확률을 나타냄
    - ex) pi=1 이면 집합 안의 모든 항목이 i속성을 가진 경우
- 엔트로피는 0 ~ 1 사이의 값
    - 순수하게(완벽) 분류되면 -> 0
    - 완벽하게 섞이면(50:50) -> 1

### 정보이득(Information Gain)
<img width="487" alt="스크린샷 2022-03-10 오전 1 24 30" src="https://user-images.githubusercontent.com/63540952/157484868-893f565f-971d-4afd-8876-229dc2f5fa77.png">

- 엔트로피는 단지 속성의 불순도를 표현
- 우리가알고자하는것 -> 어떤 속성이 얼마나 많은 정보를 제공하는가?
- 정보이득이 크다 = 어떤 속성으로 분할할 때 불순도가 줄어든다
- 모든 속성에 대해 분할한 후 정보 이득 계산
- 정보 이득이 가장 큰 속성부터 분할

## 가지치기
- max_depth 매개변수 값을 조정해 가지치기를 할 수 있음
- 학습 데이터에 대한 성능은 낮아지나 평가데이터에 대한 성능을 높일 수 있음
- 가장 적절한 max_depth값을 찾도록 노력해야함

### 시각화
- 적절한 figsize설정필요
- 불순도가 낮을 수록 진한 배경색
- feature_importances_ 속성 값을 활용하여 중요한 역할을 한 변수를 시각화해서 확인 가능

## 용어
- 뿌리 마디(Root Node) : 전체 자료를 갖는 시작하는 마디
- 자식 마디(Child Node) : 마디 하나로부터 분리된 2개 이상의 마디
- 부모 마디(Parent Node) : 주어진 마디의 상위 마디
- 끝 마디(Terminal Node) : 자식 마디가 없는 마디
- 중간 마디(Internal Node) : 부모 마디와 자식 마디가 모두 있는 마디
- 가지(Branch) : 연결되어 있는 2개 이상의 마디 집합
- 깊이(Depth) : 뿌리 마디로부터 끝 마디까지 연결된 마디 개수

