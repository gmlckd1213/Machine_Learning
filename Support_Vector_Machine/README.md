# Support Vector Machine

- 분류를 위한 기준선, 즉 결정 경계선(Decision Boundary)을 찾는 알고리즘
- SVM 성능을 높이기 위해 정규화 작업이 필요
- 분류 문제와 회귀 문ㄴ제 모두에 사용 가능(SVC, SVR)
- 본 학습에서는 분류 문제를 위한 SVM 관점에서 설명함

<img width="297" alt="image" src="https://user-images.githubusercontent.com/63540952/158008113-a8e6da2d-0902-44b5-9358-a060c46cb610.png">


## 용어
- 결정 경계(Decision Boundary)
    * 서로 다른 분류값을 결정하는 경계
- 벡터(Vector)
    * 2차원 공간 상에서 나타난 데이터 포인트
- 서포트 벡터(Support Vector)
    * 결정 경계선과 가장 가까운 데이터 포인트
- 마진(Margine)
    * 서포트 벡터와 결정경계 사이의 거리
    * 마진을 최대로 하는 결정 경계를 찾는 것이 SVM의 목표
    * 마진이 클수록 새로운 데이터에 대해 안정적으로 분류할 가능성이 높음

### 결정경계
- SVM 학습의 최종 목표는 주어진 데이터로 결정경계를 찾는 것
- 데이터 벡터 공간을 N차원이라고 할 경우 결정 경계는 N-1 차원
- 데이터가 3차원 공간에 분포할 경우 결정 경계는 2차원인 면이 됨
- 결정 경계를 초평면(Hyperplane)이라 부르기도한다.

<img width="618" alt="image" src="https://user-images.githubusercontent.com/63540952/158008673-1ccd2843-c603-473b-821c-ae941a8a9512.png">

### 비용
- 학습 시 에러가 적은 모델보다 운용 시 에러가 적은 모델이 더 좋은 모델
- SVM은 약간의 오류를 허용하기위해 *비용(C)* 이라는 변수를 사용
- 비용을 낮게 잡으면(이상치들이 있을 가능성을 크게봄)
    - 마진을 높이고 에러를 증가시키는 결정 경계선을 만듦(Soft Margine) -> 과소적합 위험
- 비용을 높게 잡으면(이상치들이 있을 가능성을 작게 봄)
    - 마진은 낮추고 에러를 감소시키는 결정 경계선을 만듦(Hard Margine) -> 과대적합 위험
- SVM에서 적절한 비용 값을 찾는 과정이 매우 중요

### 커널트릭
- 매핑 함수
    - 저차원 데이터를 고차원 데이터로 옮겨주는 함수
    - 대량 데이터를 저차원에서 고차원으로 옮기려면 계산량이 많아 사용하기 힘듦
- 커널 트릭
    - 데이터를 실제로 고차원으로 옮기지 않지만 옮긴 것과 같은 효과를 줘서 매우 빠르게 결정 경계선을 찾는 방법

#### 종류

<img width="648" alt="image" src="https://user-images.githubusercontent.com/63540952/158009175-761177f0-7439-4d18-9343-678dd85b6bab.png">

- 모델 선언 시 커널 종류를 지정함
- 가장 널리 쓰이는 커널은 rbf 커널(기본값)
- poly, rbf의 경우 gamma 값 설정에 대한 고민이 필요


### 감마
- 모델이 생성하는 경계가 복잡해지는 정도
- 값이 클 수록 작은 표준편차를 가져, 데이터 포인트가 행사하는 영향력이 줄어듦
- 값이 낮을 수록 데이터 포인터가 영향력을 행사하는 거리가 길어져 경계가 단순해짐
- c 처럼 너무 낮으면 과소적합(Underfitting), 너무 높으면 과대적합(Overfitting) 위험


## C, gamma
<img width="438" alt="image" src="https://user-images.githubusercontent.com/63540952/158009289-03bab9fb-8c7c-414e-8a94-27ea8f3f90aa.png">


